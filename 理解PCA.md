# 理解主成分分析(PCA)
## 导言
    主成分分析法(PCA)是一种常用的数据分析手段。对于一组数据的不同维度之间可能存在线性相关关系，PCA能够把这组数据通过正交变换变成各个维度之间线性无关的数据。经过PCA处理的数据中的各个样本之间的关系往往更直观，所以它是一种非常常用的数据分析和预处理工具。经过PCA处理之后的数据各个维度之间是线性无关的，通过剔除方差较小的那些维度上的数据我们可以达到数据降维的目的。在本文中，我们将介绍PCA的原理、应用以及缺陷。
## 为什么要有PCA
如果数据之中的某些维度之间存在较强的线性相关关系，那么样本在这两个维度上提供的信息有一定的重复， 所以在处理数据时我们希望数据各个维度之间是不相关的(也就是正交的)。此外，出于降低处理数据的计算量或去除噪声等目的，我们也希望能够将数据集中一些不那么重要(方差小)的维度剔除掉。例如在下图中，数据在x轴和y轴两个维度上存在着明显的相关性，当我们知道数据的x值时也能大致确定y值的分布。但是如果我们不是探究数据的x坐标和y坐标之间的关系，那么数据的x值和y值提供的信息就有较大的重复。在绿色箭头标注的方向上数据的方差较大，而在蓝色箭头方向上数据的方差较小。这时候我们可以考虑利用蓝色和绿色的箭头表示的单位向量来作为新的基底，在新的坐标系中原来不同维度间线性相关的数据变成了线性不相关的。由于在蓝色箭头方向上数据的方差较小，在需要降低数据维度的时候我们可以将这一维度上的数据丢弃并且不会损失较多的信息。如果把丢弃这一维度之后的数据重新变化回原来的坐标系，得到的数据与原来的数据之间的误差不大。这被称为重建误差最小化。PCA就是进行这种从原坐标系到新的坐标系的变换的。
![示意图](D:/PCA/application/graph.png)

## 如何计算PCA
数据经过PCA变换之后的各个维度被称为主成分，各个维度之间是线性无关的。为了使变换后的数据各个维度提供的信息量从大到小排列，变换后的数据的各个维度的方差也应该是从大到小排列的。数据经过PCA变换之后方差最大的那个维度被称为第一主成分。
我们先来考虑第一主成分如何计算。假设每一条原始数据是一个m维行向量，数据集中有n条数据。这样原始数据就可以看作一个n行m列的矩阵。我们将其称为X，用$x^{(i)}$代表数据集中的第i条数据（也就是X的第i和行向量）。这里为了方便起见，我们认为原始数据的各个维度的均值都是0。当原始数据的一些维度的均值不为0时我们首先让这一维上的数据分别减去这一维的均值以将其均值变为0。为了使X变化到另一个坐标系，我们需要让X乘以一个$m \times m$的正交变换矩阵W。W视为由列向量$<w_1, w_2, ..., w_m>$组成。我们让X和W进行矩阵相乘之后就可以原始数据变换到新的维度中。
                                $$T = XW$$
为了使变换不改变数据的大小，我们让W中的每个列向量$w_i$的长度都为0，也就是$\lVert w_i \rVert = 1$。T中的各个列向量为$<t_1, t_2, ..., t_m>$。为了使第一主成分($t_1$)的方差最大，
    $$w_1 = \arg \max_{\lVert w_1 \rVert = 1} \sum_{i = 1}^{n}(t_{1i})^2\\
    = \arg \max_{\lVert w_1 \rVert = 1} \sum_{i = 1}^{n}(x^{(i)} \cdot w_1)^2 \\
    = \arg \max_{\lVert w_1 \rVert = 1} w_1^TX^TXw_1 $$
上述最优化问题中$w_1$的长度被限制为1，为了求解$w_1$，我们将其变成如下的形式：
$$w_1 = \arg \max \frac{w_1^TX^TXw_1}{w_1^Tw_1}$$
这时候求解出的$w_1$的长度是不确定的，我们只要取长度为1的解就可以了。$$\frac{w_1^TX^TXw_1}{w_1^Tw_1}$$
是一个非常常见的瑞利熵，其更一般的形式是
$$\frac{x^TMx}{x^Tx}$$
这里的M是一个厄米特矩阵(Hermitian Matrix)，$x$是一个长度不为零的列向量。厄米特矩阵中的关于对角线对称位置上的两个元素共轭，也就是$m_{ij}= \overline{m_{ij}}$。我们把对一个矩阵A中的每个元素$a_{ij}$替换为其共轭数$\overline{a_{ij}}$，然后再进行转置称为厄米特变换，变换结果记作$A^H$。一个各个元素均为实数的厄米特矩阵就是一个对阵矩阵。这里的$X^TX$很显然是一个对称矩阵。对一个厄米特矩阵进行特征值分解，我们可以得到：
        $$M = P^HDP$$
这里的D是一个对角矩阵，对角线上的元素是特征值；$P = <p_1, p_2, ..., p_n>$，每个$p_i$都是一个长度为1的特征向量，不同的特征向量之间正交。我们将特征值分解的结果带回瑞利熵中可以得到
$$\frac{x^TMx}{x^Tx} = \frac{x^TP^HDPx}{x^TP^HPx} \\
 = \frac{\sum_{i}\lambda_i y_i^2}{\sum_{i}y_i^2}\\
  = \sum_{i}\lambda_i\frac{y_i^2}{\sum_{k}y_k^2}$$
 这里的$y_i = p_i \cdot x$。令$\alpha_i = \frac{y_i^2}{\sum_{k}y_k^2} $，这时有$\sum_{i} \alpha_i = 1$。这样$\sum_{i} \lambda_i\alpha_i$就构成了一个一维凸包。根据凸包的性质我们可以知道，当最大的$\lambda_i$对应的$\alpha_i = 1$时整个式子有最大值。所以当$x$的为最大的特征值对应的特征向量时瑞利熵有最大值，这个最大值就是最大的特征值。根据这个结论我们就可以知道$w_1$就是$X^TX$的最大的特征值对应的特征向量，第一主成分$t_1 = Xw_1$。这样我们就得到了计算第一主成分的方法，接下来我们继续考虑如何计算其他的主成分。因为$W$是一个正交矩阵，所以
 $$w_k = \arg \max \frac{w_k^TX^TXw_k}{w_k^Tw_k}\\
 s.t.\ \ w_i^Tw_k = 0,\ i < k$$
 这时候$$\frac{w_k^TX^TXw_k}{w_k^Tw_k} = \frac{\sum_{i = 1}^{m}\lambda_i (w_k^Tw_i)^2}{\sum_{i = 1}^{m}(w_k^Tw_i)^2}\\
 = \frac{\sum_{i = k}^{m}\lambda_i (w_k^Tw_i)^2}{\sum_{i = k}^{m}(w_k^Tw_i)^2}$$
 为了使第k个主成分在与前k - 1个主成分线性无关的条件下的方差最大，那么$w_k$应该是第k大的特征值对应的特征向量。经过这些分析我们就能发现变换矩阵W中的每个列向量就是$X^TX$的各个特征向量按照特征值的大小从大到小排列得到的。
接下来我们对如何计算PCA做一个总结：

 1. 把每一条数据当一个行向量，让数据集中的各个行向量堆叠成一个矩阵。
 2. 将数据集的每一个维度上的数据减去这个维度的均值，使数据集每个维度的均值都变成0，得到矩阵X。
 3. 计算方阵$X^TX$的特征值和特征向量，将特征向量按照特征值由大到小的顺序从左到右组合成一个变化矩阵W。为了降低数据维度，我们可以将特征值较小的特征向量丢弃。
 4. 计算$T = XW$，这里的T就是经过PCA之后的数据矩阵。

除了这种方法之外，我们还可以使用奇异值分解的方法来对数据进行PCA处理。
## PCA的应用
首先我们先来看一下PCA在数据降维方面的应用。我们在MNIST数据集上进行了测试。我对MNIST的测试集进行了PCA变换，对于得到的特征值的分布如下图：
![MNIST数据集特征值的分布](D:/PCA/application/eigenvalue.png)
我们可以发现特征值迅速的从较大的值迅速下降到接近0，然后不断的趋向于0。接下来我们取前200，300个特征向量对数据进行降维和重建。我们发现使用前200个主成分重建的图像已经能够大致分辨出每个数字，使用前300个主成分重建的图像已经比较清晰。根据实验我们可以发现PCA是一种比较强大的数据降维工具。
![原始图像](D:/PCA/application/orgin.png)
![使用前200个主成分重建的图像](D:/PCA/application/reconstruct200.png)
![使用前300个主成分重建的图像](D:/PCA/application/reconstruct300.png)
PCA在自然语言处理方面也有比较多的应用，其中一个应用就是用来计算词向量。word2vec是Google在2013年提出了一个自然语言处理工具包，其思想是用一个向量来表示单词，意思和词性相近的单词对应的向量之间的距离比较小，意思和词性相差较大的单词之间的距离比较大。word2vec原本是使用神经网络计算出来的，本文中的PCA也可以被用于计算词向量。具体的做法为：构建一个单词共生矩阵，然后对这个矩阵进行PCA降维，将降维得到的数据作为词向量。使用这种方法构造出的词向量在单独使用时效果可能不如使用神经网络计算出的词向量，但是将神经网络构造出来的词向量和使用PCA降维得到的词向量相加之后得到的词向量在表示词语意思时的效果要好于单独使用神经网络计算出来的词向量。
![一个共生矩阵的例子](D:/PCA/application/co-occurance.png)
## PCA的缺陷
虽然PCA是一种强大的数据分析工具，但是它也存在一定的缺陷。一方面，PCA只能对数据进行线性变换，这对于一些线性不可分的数据是不利的；另一方面，PCA的结果容易受到每一维数据的大小的影响，如果我们对每一维数据乘以一个不同的权重因子之后再进行PCA降维，得到的结果可能与直接进行PCA降维得到的结果相差比较大。除此之外，PCA要求数据每一维的均值都是0，在将原始数据的每一维都变成0时可能会丢失掉一些信息。

##参考文献
1. [Principal component analysis(主成分分析)](https://en.wikipedia.org/wiki/Principal_component_analysis).Wikipedia.
2.  Pearson, K. (1901). ["On Lines and Planes of Closest Fit to Systems of Points in Space"](http://stat.smmu.edu.cn/history/pearson1901.pdf) (PDF). Philosophical Magazine. 2 (11): 559–572.
3.  [Rayleigh quotient(瑞利熵)](https://en.wikipedia.org/wiki/Rayleigh_quotient).Wikipedia. 
4.   [Hermitian matrix(厄米特矩阵)](https://en.wikipedia.org/wiki/Hermitian_matrix).Wikipedia.
5. Yann LeCun. [MNIST数据集](http://yann.lecun.com/exdb/mnist/).
6. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.(2013) ["Distributed Representations of Words and Phrases and their Compositionality"](https://arxiv.org/pdf/1310.4546). arxiv.org.